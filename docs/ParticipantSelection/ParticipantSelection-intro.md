# Selecting Participants for Hack Weeks

All of our hack weeks have been oversubscribed by at least a factor of 2. This brings up an 
important question: who do we invite to the hack week, and who do we leave out? 
The interactive nature of a hack week necessitates a careful procedure for participant selection.
Selection committees will likely want to pick participants who are on board with and willing to 
advance the vision and values of the hack week model around collaboration and inclusivity.
It may be tempting and straightforward to select candidates that are known personally to the committee
as good citizens of the community, but we warn organizers that taking this approach might 
run counter several goals that a hack week might have: organizers should ask themselves how diverse 
their own networks are in practice. Do these networks contain predominantly researchers from 
privileged backgrounds and/or institutions? If so, they might want to consider strategies to broaden 
their reach and draw in researchers from outside of their networks.  

## General Notes on the Selection Strategy

Participant selection is one of the most involved and fraught, but also often overlooked aspects 
of organizing a participant-driven workshop. Organizers should start the process *early*, ideally 
at the same time as the team formulates their goals for organizing the hack week: formulating a 
strategy for selecting participants *starts* with a formulation of the goals and
objectives of the workshop, along with the core values that organizers are aiming to implement and 
advance. From there, organizers may ask how participant selection might serve these goals and values. 
Possible goals are:
* teaching data science methods to a wide range of researchers in your domain
* encouraging collaborations between researchers of different fields
* encouraging collaborations between researchers in different stages of their careers
* improving access to modern methods and tools for researchers from underrepresented groups

These are just examples: the goals of your workshop will necessarily depend on the particular community
the hack week is embedded in and its needs. In the ideal case, the formulated goals lead to a 
clear strategy for selecting participants.
For example, a workshop that targets early-career researchers and is more teaching-oriented may wish 
to preferentially admit graduate students and postdocs, whereas a workshop aimed at collaboration across 
career stages may prefer to mix career stages more broadly.
In practice, however, it is often not trivial to match up workshop goals with specific choices to be 
made

Similarly, organizers should articulate a strategy for assessing whether candidates have the appropriate 
skill level and are likely to participate in a way that creates a welcoming community for the group 
as a whole. This is generally a hard problem: research from the hiring literature suggests that there 
is an irreducible variance when trying to predict future performance from applications and intervies 
(see e.g. [Highhouse, 2008]()). This is also the place where unconscious biases are most likely to creep 
into the process. Our best advice is to think critically about the selection process, ensure that all 
committee members are aware of common unconscious biases and are reminded of them throughout the selection 
process, and to evaluate and re-evaluate every part of the selection process before, during and after 
each workshop. For all of our hack weeks, we have been continuously learning and aiming to improve 
each year, and we continue to incorporate our own observations and external feedback in our workshops.

## Designing and Assessing Application Forms

Because our hack weeks are oversubscribed, we generally ask participants to *apply* using a form, and 
then go through an internal selection procedure to select participants from among the applicants. 
What information candidates are asked to provide during the application differs quite widely 
among the hack weeks, but for all hack weeks (and indeed all workshops), it is important to keep 
in mind that **one can only use information for a selection that has been collected on the application 
form**. This may seem obvious, but experience has taught us in several instances that omissions on 
the form led to serious difficulties in our selection later. Especially with questions that aim to 
probe values and traits such as collaborativeness, designing questions that elicit useful information 
about the trait in question is difficult, and on many occasions a question we thought would do so 
did not, in fact, provide us with useful information.

It is therefore worth the time and effort to design application forms carefully and intentionally.
In the process, especially for open-ended questions, organizers should interrogate their own 
expectations about how participants might answer, and their biases in the answers that are expected.
For example, a bias against non-native English speakers might lead reviewers to rate responses by
these candidates lower. Similarly, for questions around diversity and inclusion, it is worth considering 
how differences across cultures and countries might affect what terminology and concepts candidates 
may be familiar with. There may also be effects related to seniority and familiarity with 
university environments and culture: more senior participants, especially those from more privileged 
institutions, might have a better sense of what information organizers are trying to elicit, simply 
because they have had more training in responding to questions like those likely asked on hack week 
application forms, and because they have been embedded in the particular culture of academic departments. 

While many of the hack weeks use Google Forms for application forms, it is worth critically 
examining that choice on the basis of respondent privacy and data rights, especially when 
forms may ask for sensitive demographic information.

### Assessing Qualifications

Some hack weeks have requirements about the proficiency with programming or tools that participants 
are expected to have at the start of the hack week. Proficiency could be assessed, as in admission 
contexts, through university transcripts. However, grades do not necessarily reflect proficiency, 
and the grading systems employed in different countries may make an assessment difficult. 
Organizers could also simply *ask* participants whether they believe themselves to be at a certain 
skill levels, e.g. whether they are beginners, intermediate users or experts. Because there is no 
objective scale for what an "expert programmer" might be, and different people might give different 
answers to that question, assessing proficiency that way may lead to biases in the selection.

In the hack weeks, we generally aim to assess proficiency with questions that tie skills to 
particular milestone achievements. For example, when asking about proficiency with machine learning, 
we might ask participants whether they've only encountered machine learning in their course work, 
whether they have used a machine learning algorithm for a research project, or whether they have 
developed and implemented machine learning algorithms themselves. One goal here is to make the 
questions as clear and unambiguous as possible. 
However, even here biases may affect the selection. In particular, participants from institutions or 
countries that are less well-resources may be less likely to have encountered computational classes 
or trainings in their home institutions. As a result, they may be less likely to be proficient 
programmers, and simply selecting on coding ability may select out these participants. 
We suggest that organizers view these questions in the larger context and take the opportunities that 
a particular applicant may or may not have had into account during the selection process.

### Assessing Core Values

Hack weeks thrive through participants who are enthusiastic, curious collaborative and kind to others. 
Assessing these traits is perhaps the most difficult part of any selection procedure. In addition, 
some hack weeks have asked questions around the contributions that participants are likely to make, 
and about the impact that someone's participation might have on their local community. Our hack weeks 
have assessed these values and traits through a range of different methods, including open-ended 
questions on the application form, personal statements and recommendation letters.
As mentioned above, questions should be phrased thoughtfully and intentionally with the goal of 
minimizing biases during this stage.
Following best practices from the hiring literature (e.g. [Bohnet, 2015]()), these responses, letters 
or statements should be graded by several reviewers, using a clear, unambiguous set of rubrics that have 
been vetted carefully to minimize biases. Some hack weeks have performed this grading using a blinded 
set, where the names and demographics of the applicants were hidden from the raters. Other hack weeks 
have instead taken other information, for example career stage, home institution and demographic information
into account at this stage in order to mediate effects that may systematically disadvantage some candidates.
Based on our current knowledge as organizers, there is no single best way to do this: while there are 
well-documented effects that indicate that certain candidates may be disadvantaged based on their name 
alone in hiring contexts (STUDY), some research also shows that committees that are careful about taking
systematic effects and biases into account tend to do *worse* when demographic information about 
the candidates is removed (ALSO STUDY). 

Once all applications have been rated, the members of the organizing committee might then 
take the ratings and carefully examine them for biases. For example, one may look at interrate reliability 
to see whether specific raters tend to only give grades in a certain subset of the available ratings. 
One may also look at whether certain groups (for example junior academics) are systematically rated worse, 
indicating that biases have not been fully eliminated during earlier stages of the process. Data visualizations 
are enormously helpful here, as is a practice of multiple organizers taking an independent look at the 
ratings generated by the reviewers.

### Questions about the Participant's Background

For all hack weeks, considerable thought and discussion goes into which information to elicit regarding 
the applicants' backgrounds. This includes for example their current institution, academic field of study 
and/or research, and career stage, but also questions around gender identity, sexual orientation and race and ethnicity.
In particular sensitive questions around one's identity are difficult to phrase and incorporate in selection 
procedures. Organizers may decide to simply not include them. However, if the pool of applicants is biased 
with respect to those categories, organizers risk that the workshop itself may reproduce those biases, or 
exacerbate them if other categories of assessment (for example open-ended answers) correlate with demographic 
categories. 



Because no procedure, no matter how well designed, will be entirely bias-free and a hundred percent 
predictive, ranking participants and admitting according to ranking is unlikely to produce a balanced 
and unbiased cohort. It may be more advisable to set a reasonable cut-off score of some form (for example, 
all participants that are acceptable must at least have been given a score of 3 out of five by all 
reviewers), and then select within that set of acceptable candidates according to other criteria important 
to the workshop (for example, as mentioned in the example above, the mix of career stage).
It may make sense to pre-select a subset of participants at this stage that all reviewers strongly agree on,
but again we recommend against selecting *all* participants this way. 

A tool that we have successfully used in our hack weeks for the latter step--selection of the cohort 
out of a set of acceptable candidates, subject to multiple constraints--is called [Entrofy](). 
The accompanying notebook shows an example procedure for cohort selection, using a simulated data 
set. Organizers are encouraged to adapt this notebook to their own needs.


### General Questions

* name
* e-mail address (don't forget this! you need to contact your candidates somehow!)
* institution

### Questions Probing Motivation and 


### Demographic Questions





 




